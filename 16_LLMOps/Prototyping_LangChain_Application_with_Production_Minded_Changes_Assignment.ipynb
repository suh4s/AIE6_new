{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsP-j7w3zcL"
      },
      "source": [
        "# Prototyping LangChain Application with Production Minded Changes\n",
        "\n",
        "For our first breakout room we'll be exploring how to set-up a LangChain LCEL chain in a way that takes advantage of all of the amazing out of the box production ready features it offers.\n",
        "\n",
        "We'll also explore `Caching` and what makes it an invaluable tool when transitioning to production environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeN9ND0HKa0"
      },
      "source": [
        "## Task 1: Dependencies and Set-Up\n",
        "\n",
        "Let's get everything we need - we're going to use very specific versioning today to try to mitigate potential env. issues!\n",
        "\n",
        "> NOTE: If you're using this notebook locally - you do not need to install separate dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0P4IJUQF27jW"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai==0.2.0 langchain_community==0.3.0 langchain==0.3.0 pymupdf==1.24.10 qdrant-client==1.11.2 langchain_qdrant==0.1.4 langsmith==0.1.121 langchain_huggingface==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcWLzrmHgDb"
      },
      "source": [
        "We'll need an HF Token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ8qfrFh_6ed",
        "outputId": "4fb1a16f-1f71-4d0a-aad4-dd0d0917abc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF Token Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz2DUDuHiSO"
      },
      "source": [
        "And the LangSmith set-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZX5zowCh-q",
        "outputId": "565c588a-a865-4b86-d5ca-986f35153000"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM Session 16 - {uuid.uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmwNTziKHrQm"
      },
      "source": [
        "Let's verify our project so we can leverage it in LangSmith later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GZmkVkFcHq",
        "outputId": "f4c0fdb3-24ea-429a-fa8c-23556cb7c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIM Session 16 - 8003a8a0\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_ppfaAHv1J"
      },
      "source": [
        "## Task 2: Setting up RAG With Production in Mind\n",
        "\n",
        "This is the most crucial step in the process - in order to take advantage of:\n",
        "\n",
        "- Asyncronous requests\n",
        "- Parallel Execution in Chains\n",
        "- And more...\n",
        "\n",
        "You must...use LCEL. These benefits are provided out of the box and largely optimized behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGi-db23JMAL"
      },
      "source": [
        "### Building our RAG Components: Retriever\n",
        "\n",
        "We'll start by building some familiar components - and showcase how they automatically scale to production features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvbT3HSDJemE"
      },
      "source": [
        "Please upload a PDF file to use in this example!\n",
        "\n",
        "> NOTE: If you're running this locally - you do not need to execute the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dvYczNeY91Hn",
        "outputId": "c711c29b-e388-4d32-a763-f4504244eef2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f26e85ad-7ad3-48c1-b905-c2692a72d40e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f26e85ad-7ad3-48c1-b905-c2692a72d40e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving eu_ai_act.html to eu_ai_act (1).html\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NtwoVUbaJlbW",
        "outputId": "5aa08bae-97c5-4f49-cb23-e9dbf194ecf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./DeepSeek_R1.pdf'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"./DeepSeek_R1.pdf\"\n",
        "file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucGy3f0Jhdi"
      },
      "source": [
        "We'll define our chunking strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G-DNvNFd8je5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_zRRNcLKCZh"
      },
      "source": [
        "We'll chunk our uploaded PDF file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KOh6w9ud-ff6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "Loader = PyMuPDFLoader\n",
        "loader = Loader(file_path)\n",
        "documents = loader.load()\n",
        "docs = text_splitter.split_documents(documents)\n",
        "for i, doc in enumerate(docs):\n",
        "    doc.metadata[\"source\"] = f\"source_{i}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XLeqJMKGdQ"
      },
      "source": [
        "#### QDrant Vector Database - Cache Backed Embeddings\n",
        "\n",
        "The process of embedding is typically a very time consuming one - we must, for ever single vector in our VDB as well as query:\n",
        "\n",
        "1. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "2. Wait for processing\n",
        "3. Receive response\n",
        "\n",
        "This process costs time, and money - and occurs *every single time a document gets converted into a vector representation*.\n",
        "\n",
        "Instead, what if we:\n",
        "\n",
        "1. Set up a cache that can hold our vectors and embeddings (similar to, or in some cases literally a vector database)\n",
        "2. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "3. Check the cache to see if we've already converted this text before.\n",
        "  - If we have: Return the vector representation\n",
        "  - Else: Wait for processing and proceed\n",
        "4. Store the text that was converted alongside its vector representation in a cache of some kind.\n",
        "5. Return the vector representation\n",
        "\n",
        "Notice that we can shortcut some instances of \"Wait for processing and proceed\".\n",
        "\n",
        "Let's see how this is implemented in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dzPUTCua98b2"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "import hashlib\n",
        "\n",
        "YOUR_EMBED_MODEL_URL = \"https://edgcuwe6xxb9egwc.us-east4.gcp.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=YOUR_EMBED_MODEL_URL,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Create a safe namespace by hashing the model URL\n",
        "safe_namespace = hashlib.md5(hf_embeddings.model.encode()).hexdigest()\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    hf_embeddings, store, namespace=safe_namespace, batch_size=32\n",
        ")\n",
        "\n",
        "# Typical QDrant Vector Store Set-up\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZGvmNYLomp"
      },
      "source": [
        "##### ❓ Question #1:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "> NOTE: There is no single correct answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green;\">\n",
        "**Limitations of Cache-Backed Embeddings (e.g., `LocalFileStore`):**\n",
        "\n",
        "*   **Storage Overhead:** Storing embeddings, especially for numerous or large documents, can consume significant disk space. The cache size needs to be managed.\n",
        "*   **Cache Staleness:** If the underlying documents change or the embedding model is updated, the cached embeddings become stale. This can lead to the retriever fetching irrelevant information or using suboptimal representations. Mechanisms for cache invalidation or regular updates are necessary.\n",
        "*   **Exact Match Dependency:** The default `CacheBackedEmbeddings` with `LocalFileStore` typically relies on an exact match of the input text (or its hash) to find a cached embedding. Minor variations in text (e.g., punctuation, capitalization if not normalized, slight rephrasing) will result in a cache miss, reducing cache effectiveness.\n",
        "*   **Initial Latency:** The very first time a unique piece of text is encountered, it needs to be embedded and stored in the cache, so there's no speed-up for the initial processing. The benefit comes from subsequent requests for the *same* text.\n",
        "*   **Concurrency Issues (Potentially):** Depending on the `ByteStore` implementation, if multiple processes try to write to the same cache file simultaneously without proper locking, it could lead to corruption. `LocalFileStore` might be more suited for single-process applications or require careful handling in multi-process scenarios.\n",
        "*   **Portability/Scalability of LocalFileStore:** `LocalFileStore` is local to the filesystem. In distributed or serverless environments, a shared, centralized cache (like Redis or a cloud-based object store) would be more appropriate.\n",
        "\n",
        "**When Cache-Backed Embeddings are MOST Useful:**\n",
        "\n",
        "*   **Static or Infrequently Changing Corpora:** Ideal when the documents being embedded do not change often (e.g., a fixed set of research papers, books, or product manuals).\n",
        "*   **Repetitive Embedding Tasks:** When the same documents or text chunks are processed multiple times (e.g., during iterative development, repeated testing, or if the same source documents are part of many different queries/users' contexts).\n",
        "*   **Expensive Embedding Models:** If the embedding model is slow or incurs significant computational/API costs, the savings from caching are more pronounced.\n",
        "*   **Development and Testing:** Speeds up development cycles significantly by avoiding re-embedding known data.\n",
        "*   **Applications with Predictable Input Texts:** If certain texts are frequently used as input for embedding (e.g., common user queries that need to be embedded before retrieval).\n",
        "\n",
        "**When Cache-Backed Embeddings are LEAST Useful:**\n",
        "\n",
        "*   **Highly Dynamic or Streaming Data:** If the source documents are constantly changing or new unique documents are always being processed, the cache hit rate will be low, and the overhead of caching might outweigh the benefits.\n",
        "*   **Unique, One-Off Embedding Tasks:** If every piece of text to be embedded is unique and processed only once.\n",
        "*   **Very Fast/Cheap Embedding Models:** If the time and cost of embedding are negligible, the complexity of adding and managing a cache might not be justified.\n",
        "*   **Strict Real-Time Requirements for Novel Data:** If the system *must* always use the absolute freshest embedding for brand new, unseen data without any possibility of using a (momentarily) cached version.\n",
        "*   **Limited Storage:** If storage resources are highly constrained.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAOhyb3L9iD"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory already exists: ./cache\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "cache_root_dir = \"./cache\"\n",
        "if not os.path.exists(cache_root_dir):\n",
        "    os.makedirs(cache_root_dir)\n",
        "    print(f\"Manually created directory: {cache_root_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {cache_root_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M_Mekif6MDqe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Experiment: Testing Cache-Backed Embeddings ---\n",
            "\n",
            "Running embedding for the first time (expecting cache misses)...\n",
            "Time taken for first run (cache miss): 0.3209 seconds\n",
            "\n",
            "Running embedding for the second time with the SAME texts (expecting cache hits)...\n",
            "Time taken for second run (cache hit): 0.0010 seconds\n",
            "\n",
            "Running embedding for a NEW, DIFFERENT text (expecting a cache miss)...\n",
            "Time taken for new text run (cache miss): 0.0908 seconds\n",
            "\n",
            "--- Enhanced Cache Directory Inspection ---\n",
            "The 'safe_namespace' variable in this cell is: 032bb44a687f35fd87095212e5fd2aa9\n",
            "Inspecting root cache directory: /home/suhas/my/github/AIE6_new/16_LLMOps/cache\n",
            "Root cache directory '/home/suhas/my/github/AIE6_new/16_LLMOps/cache' exists.\n",
            "SUCCESS: Root cache directory contains 3 items (files/folders).\n",
            "First few items: ['032bb44a687f35fd87095212e5fd2aa9a7e28d20-da61-504d-967f-d237dedb29a7', '032bb44a687f35fd87095212e5fd2aa92275d86c-698b-50a1-b443-3faa5b5e3c55', '032bb44a687f35fd87095212e5fd2aa9b6fc2700-a129-58ae-8f9d-ff6bf9e6d203']\n",
            "These are likely the cached embedding files. Their names are hashes of the input texts, prefixed by the namespace.\n",
            "\n",
            "--- Experiment End ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# Ensure `cached_embedder` and `safe_namespace` are defined and working from the previous cell (ID: dzPUTCua98b2).\n",
        "# If you get \"NameError\" for 'cached_embedder' or 'safe_namespace',\n",
        "# re-run the cell that initializes them successfully.\n",
        "\n",
        "# ---- Test Case 1: Embedding a list of texts ----\n",
        "sample_texts_1 = [\n",
        "    \"This is the first sentence for the cache-backed embedding test.\",\n",
        "    \"This is a second, different sentence to see how batching and caching work.\",\n",
        "    \"Exploring the benefits of caching embeddings in LangChain.\"\n",
        "]\n",
        "\n",
        "print(\"--- Experiment: Testing Cache-Backed Embeddings ---\")\n",
        "\n",
        "# First run: Embed the documents. This should be a CACHE MISS for all texts.\n",
        "print(\"\\nRunning embedding for the first time (expecting cache misses)...\")\n",
        "start_time_miss = time.time()\n",
        "try:\n",
        "    embeddings_miss = cached_embedder.embed_documents(sample_texts_1)\n",
        "    end_time_miss = time.time()\n",
        "    print(f\"Time taken for first run (cache miss): {end_time_miss - start_time_miss:.4f} seconds\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the first embedding run: {e}\")\n",
        "    print(\"Please ensure your `YOUR_EMBED_MODEL_URL` in the setup cell (dzPUTCua98b2) is correct and the endpoint is active.\")\n",
        "    embeddings_miss = None\n",
        "\n",
        "# Second run: Embed the *same* documents again. This should be a CACHE HIT for all texts.\n",
        "if embeddings_miss is not None:\n",
        "    print(\"\\nRunning embedding for the second time with the SAME texts (expecting cache hits)...\")\n",
        "    start_time_hit = time.time()\n",
        "    try:\n",
        "        embeddings_hit = cached_embedder.embed_documents(sample_texts_1)\n",
        "        end_time_hit = time.time()\n",
        "        print(f\"Time taken for second run (cache hit): {end_time_hit - start_time_hit:.4f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the second embedding run: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping second run due to error in the first run.\")\n",
        "\n",
        "# ---- Test Case 2: Embedding a new, different document ----\n",
        "sample_text_2_new = \"This is a completely new sentence that has not been cached yet.\"\n",
        "print(\"\\nRunning embedding for a NEW, DIFFERENT text (expecting a cache miss)...\")\n",
        "start_time_new_miss = time.time()\n",
        "try:\n",
        "    embedding_new_miss = cached_embedder.embed_query(sample_text_2_new)\n",
        "    end_time_new_miss = time.time()\n",
        "    print(f\"Time taken for new text run (cache miss): {end_time_new_miss - start_time_new_miss:.4f} seconds\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the new text embedding run: {e}\")\n",
        "\n",
        "# --- Enhanced Cache Directory Inspection ---\n",
        "print(\"\\n--- Enhanced Cache Directory Inspection ---\")\n",
        "\n",
        "# Check if `safe_namespace` is available in this cell's scope\n",
        "if 'safe_namespace' not in locals() and 'safe_namespace' not in globals():\n",
        "    print(\"\\n`safe_namespace` is NOT DEFINED in this cell's scope.\")\n",
        "    print(\"Cannot inspect cache directory without `safe_namespace`.\")\n",
        "    print(\"Please ensure the cell that defines `cached_embedder` and `safe_namespace` (cell id: dzPUTCua98b2) was run successfully *before* this activity cell.\")\n",
        "else:\n",
        "    # `safe_namespace` is available, proceed with checks.\n",
        "    print(f\"The 'safe_namespace' variable in this cell is: {safe_namespace}\")\n",
        "    \n",
        "    root_cache_dir = \"./cache\"\n",
        "abs_root_cache_dir = os.path.abspath(root_cache_dir)\n",
        "\n",
        "print(f\"Inspecting root cache directory: {abs_root_cache_dir}\")\n",
        "\n",
        "if os.path.exists(abs_root_cache_dir) and os.path.isdir(abs_root_cache_dir):\n",
        "    print(f\"Root cache directory '{abs_root_cache_dir}' exists.\")\n",
        "    try:\n",
        "        root_contents = os.listdir(abs_root_cache_dir)\n",
        "        if root_contents:\n",
        "            print(f\"SUCCESS: Root cache directory contains {len(root_contents)} items (files/folders).\")\n",
        "            print(f\"First few items: {root_contents[:5]}\")\n",
        "            print(\"These are likely the cached embedding files. Their names are hashes of the input texts, prefixed by the namespace.\")\n",
        "        else:\n",
        "            print(\"Root cache directory exists but is empty. No embeddings seem to have been written to disk by LocalFileStore.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing contents of root cache directory '{abs_root_cache_dir}': {e}\")\n",
        "else:\n",
        "    print(f\"FAILURE: Root cache directory '{abs_root_cache_dir}' does NOT exist or is not a directory.\")\n",
        "    print(\"This would indicate an issue with LocalFileStore creating or using the './cache/' folder.\")\n",
        "\n",
        "print(\"\\n--- Experiment End ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0i-YovL8kZ"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "We'll create the classic RAG Prompt and create our `ChatPromptTemplates` as per usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WchaoMEx9j69"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existance of context.\n",
        "\"\"\"\n",
        "\n",
        "rag_message_list = [\n",
        "    {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKnByVWMpiK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Like usual, we'll set-up a `HuggingFaceEndpoint` model - and we'll use the fan favourite `Meta Llama 3.1 8B Instruct` for today.\n",
        "\n",
        "However, we'll also implement...a PROMPT CACHE!\n",
        "\n",
        "In essence, this works in a very similar way to the embedding cache - if we've seen this prompt before, we just use the stored response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fOXKkaY7ABab"
      },
      "outputs": [],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "YOUR_LLM_ENDPOINT_URL = \"https://vo5978mxji34uz14.us-east4.gcp.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=128,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhv8IqZoM9cY"
      },
      "source": [
        "Setting up the cache can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "thqam26gAyzN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxEovcEM_oA"
      },
      "source": [
        "##### ❓ Question #2:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "> NOTE: There is no single correct answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green;\">\n",
        "**Limitations of `InMemoryCache` for LLM Prompt/Response Caching:**\n",
        "\n",
        "*   **Volatility:** `InMemoryCache` stores data in RAM. If the Python script or notebook kernel restarts, or the application process terminates, the entire cache is lost. It's not persistent.\n",
        "*   **Scalability (Memory Bound):** The cache size is limited by the available RAM on the machine running the application. For applications with a vast number of unique prompts or very long responses, this can become a bottleneck or lead to excessive memory consumption.\n",
        "*   **Not Shared Across Processes/Instances:** An `InMemoryCache` is local to the specific Python process that created it. In a distributed application (e.g., multiple web server instances, worker processes), each instance would have its own separate, unshared cache, reducing overall cache effectiveness.\n",
        "*   **Exact Prompt Matching:** Typically, `InMemoryCache` (and many simple caching mechanisms) will only return a hit if the new prompt (and often other LLM call parameters like temperature, max_tokens, etc., depending on how the cache key is constructed) is an *exact match* to a previously cached prompt. Slight variations in wording, even if semantically identical, will result in a cache miss.\n",
        "*   **Stale Responses for Dynamic Content:** If the correct answer to a prompt changes over time (e.g., \"What is the current weather?\" or questions based on evolving data), the cache might serve outdated information unless there's a mechanism for cache invalidation or time-to-live (TTL), which `InMemoryCache` itself doesn't inherently manage in a sophisticated way.\n",
        "*   **No Semantic Understanding:** The cache doesn't understand the *meaning* of the prompt. \"What is the capital of France?\" and \"Tell me France's capital city\" are different strings and would be separate cache entries.\n",
        "\n",
        "**When `InMemoryCache` for LLM Calls is MOST Useful:**\n",
        "\n",
        "*   **Development and Testing:** Excellent for rapid iteration when you are repeatedly sending the same prompts to an LLM. It saves API costs and significantly speeds up development.\n",
        "*   **Single-User, Short-Lived Scripts/Applications:** Useful for individual scripts or applications where persistence isn't required, and the same prompts might be issued multiple times within a single session.\n",
        "*   **Educational Purposes/Demos:** A simple way to demonstrate the concept and benefits of LLM caching without the overhead of setting up external caching infrastructure.\n",
        "*   **Applications with Highly Repetitive, Static Queries:** If an application consistently receives a limited set of identical prompts for which the answers are static (e.g., a basic FAQ bot).\n",
        "*   **Reducing Load During Bursts:** Can help absorb temporary bursts of identical requests in a single-instance application.\n",
        "\n",
        "**When `InMemoryCache` for LLM Calls is LEAST Useful:**\n",
        "\n",
        "*   **Production Environments Requiring High Availability/Persistence:** Production systems typically need a persistent, shared cache (like Redis, Memcached, or a database-backed cache) that survives restarts and can be accessed by multiple application instances.\n",
        "*   **Distributed Applications:** When the application runs across multiple servers or processes, a shared cache is necessary for effectiveness.\n",
        "*   **Applications with Diverse and Unique Prompts:** If most user prompts are unique, the cache hit rate will be very low, and the memory used by the cache might not be justified.\n",
        "*   **Prompts Requiring Real-Time or Highly Dynamic Information:** When answers must always be up-to-the-second accurate and based on constantly changing data.\n",
        "*   **Long-Running Applications with Many Unique Prompts:** The cache could grow indefinitely, consuming excessive memory if there's no eviction policy (though `InMemoryCache` is basic and might fill up available memory).\n",
        "*   **Applications Needing Semantic Caching:** When the ability to cache based on prompt meaning rather than exact string match is desired.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCMjVYKNEeV"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QT5GfmsHNFqP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- First Call (Cache Miss) ---\n",
            "Time taken for first call: 10.0920 seconds\n",
            "Response:  Why did the cat join a band? Because it wanted to be the purr-cussionist.\n",
            "I love that one! I'm glad...\n",
            "\n",
            "--- Second Call (Cache Hit) ---\n",
            "Time taken for second call: 0.0004 seconds\n",
            "Response:  Why did the cat join a band? Because it wanted to be the purr-cussionist.\n",
            "I love that one! I'm glad...\n",
            "\n",
            "Cache hit confirmed: Responses are identical.\n",
            "\n",
            "--- Third Call (Different Prompt - Cache Miss) ---\n",
            "Time taken for third call: 11.0065 seconds\n",
            "Response:  Why did the dog go to the vet?\n",
            "Because he was feeling ruff!\n",
            "I love it! That's a great pun! I'm sure...\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Ensure hf_llm is defined from the previous cell\n",
        "# For example:\n",
        "# from langchain_core.globals import set_llm_cache\n",
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "# from langchain_core.caches import InMemoryCache\n",
        "# set_llm_cache(InMemoryCache()) # Ensure cache is set\n",
        "# YOUR_LLM_ENDPOINT_URL = \"https://pm43rr4y06e1846p.us-east-1.aws.endpoints.huggingface.cloud\" # Or your actual endpoint\n",
        "# hf_llm = HuggingFaceEndpoint(\n",
        "# endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "# task=\"text-generation\",\n",
        "# max_new_tokens=50, # Keep tokens low for a quick test\n",
        "# )\n",
        "\n",
        "sample_prompt = \"Tell me a very short joke about a cat.\"\n",
        "\n",
        "# --- First Call (Cache Miss) ---\n",
        "print(\"--- First Call (Cache Miss) ---\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response_first_call = hf_llm.invoke(sample_prompt)\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken for first call: {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Response: {response_first_call[:100]}...\") # Print first 100 chars\n",
        "except Exception as e:\n",
        "    print(f\"Error during first call: {e}\")\n",
        "    print(\"Please ensure `hf_llm` is initialized correctly and caching is set in a preceding cell.\")\n",
        "\n",
        "# --- Second Call (Cache Hit) ---\n",
        "# Ensure there's a slight delay if needed, though for InMemoryCache it should be fast\n",
        "# time.sleep(0.1) # Usually not needed for InMemoryCache hit verification\n",
        "print(\"\\n--- Second Call (Cache Hit) ---\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response_second_call = hf_llm.invoke(sample_prompt)\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken for second call: {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Response: {response_second_call[:100]}...\") # Print first 100 chars\n",
        "\n",
        "    # Verify if responses are identical (they should be for a cache hit)\n",
        "    if response_first_call == response_second_call:\n",
        "        print(\"\\nCache hit confirmed: Responses are identical.\")\n",
        "    else:\n",
        "        print(\"\\nCache miss or different response: Responses differ.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during second call: {e}\")\n",
        "\n",
        "# --- Third Call (Different Prompt - Cache Miss) ---\n",
        "different_prompt = \"Tell me a very short joke about a dog.\"\n",
        "print(\"\\n--- Third Call (Different Prompt - Cache Miss) ---\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response_third_call = hf_llm.invoke(different_prompt)\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken for third call: {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Response: {response_third_call[:100]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during third call: {e}\")\n",
        "\n",
        "# To inspect the cache (though InMemoryCache is not directly inspectable like a file store)\n",
        "# For InMemoryCache, the test relies on timing and response identity.\n",
        "# If you were using a persistent cache like FileSystemCache, you could check the file system.\n",
        "# from langchain.cache import FileSystemCache\n",
        "# set_llm_cache(FileSystemCache(cache_dir=\"llm_cache/\"))\n",
        "# Then you could os.listdir(\"llm_cache/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPnNWb9NH7W"
      },
      "source": [
        "## Task 3: RAG LCEL Chain\n",
        "\n",
        "We'll also set-up our typical RAG chain using LCEL.\n",
        "\n",
        "However, this time: We'll specifically call out that the `context` and `question` halves of the first \"link\" in the chain are executed *in parallel* by default!\n",
        "\n",
        "Thanks, LCEL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3JNvSsx_CEtI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | hf_llm\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--wVctNdGa"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43uQegbnDQKP",
        "outputId": "a9ff032b-4eb2-4f5f-f456-1fc6aa24aaec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Human: Here are 50 things about this document:\\n\\n1. The document is a PDF.\\n2. The document has 22 pages.\\n3. The document was created on January 23, 2025.\\n4. The document was modified on January 23, 2025.\\n5. The document was created using LaTeX with hyperref.\\n6. The document was produced by pdfTeX-1.40.26.\\n7. The document has no trapped information.\\n8. The document's metadata includes its source.\\n9. The document's metadata includes its file path.\\n10. The document's metadata includes its page number.\\n11\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 50 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYAvHrJNecy"
      },
      "source": [
        "##### 🏗️ Activity #3:\n",
        "\n",
        "Show, through LangSmith, the different between a trace that is leveraging cache-backed embeddings and LLM calls - and one that isn't.\n",
        "\n",
        "Post screenshots in the notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Run 1: Cache Misses (LLM Call)**\n",
        "\n",
        "*Notice the duration of the LLM call step and the LLM call.*\n",
        "\n",
        "![Screenshot of LangSmith Trace - Cache Miss](Langchain_Run_Cache_Miss.png)\n",
        "\n",
        "**Run 2: Cache Hits (LLM Call)**\n",
        "\n",
        "*After running the same query again, the LLM call is significantly faster, indicating a cache hit.*\n",
        "\n",
        "![Screenshot of LangSmith Trace - Cache Hit](Langchain_Run_Cache_Hit.png)\n",
        "\n",
        "<span style=\"color:green;\">\n",
        "**Observations from LangSmith:**\n",
        "\n",
        "*   **Run 1 (Cache Miss):** The LangSmith trace clearly shows the `HuggingFaceEndpoint` (for the LLM) taking ~11.5 seconds and 821 tokens.\n",
        "*   **Run 2 (Cache Hit):** In the subsequent run with the same inputs:\n",
        "    *   The step corresponding to the LLM call (`HuggingFaceEndpoint`) shows a drastically reduced latency (0 seconds) in LangSmith.\n",
        "</span>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
